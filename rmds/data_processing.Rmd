---
title: "smartchem_raw_to_db"
author: "Keith Birchfield"
date: "January 2, 2019"
output: 
  html_document:
    toc: TRUE
    df_print: paged
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
```

## Description

This Rmd is a record of the data processing for raw data files output from SmartChem 200 software. The goal is to prepare data for import into the pnw_lakes_data database. Preparation includes evaluation of quality control metrics and notes about any changes to the data set. 

### Samples in Data Set

This data set includes the following samples, all from Lacamas Lake:

  * 2017 Inlet/Outlet
  * 2018 profile samples from Maia Kawamura's project
  * 2018 DW samples matched with Tripod sampling

## Load required packages and functions

```{r}
library(tidyverse)
library(lubridate)
library(readxl)
library(xlsx)
library(cowplot)
library(plotly)
source(file.path("functions", "smartchem_rawprocessor.R"))
source(file.path("functions", "sc_qc_multiplot_fun.R"))
source(file.path("functions", "sc_cal_curve_plot.R"))
source(file.path("functions", "sc_controls_plot.R"))
source(file.path("functions", "sc_absdups_plot.R"))
source(file.path("functions", "sc_no3coileffic_plot.R"))
```

## Import data

Make a list of file names in the raw_data folder

```{r}
filenames_raw <-
  list.files(file.path(".","raw_data"), pattern = "*\\.XLS", ignore.case = T, full.names = F)
filenames_raw
```

Specify the run dates for each file in filenames.orig and create a dataframe of filenames and their associated rundates
```{r}
run_dates_df <- as_tibble(list(filenames = filenames_raw, 
                               run_dates  = date(c("2018-08-21", "2018-08-21", "2018-08-22", "2018-08-23"))))
run_dates_df
```

## WNHA - Ammonia analysis data processing

### Import and Process excel workbook

```{r}
nh3_run <- smartchem_rawprocessor(filename = filenames_raw[1])
nh3_run
```

### Quality Control Checks

1. Calibration curve

```{r}
sc_cal_curve_plot(nh3_run)
```

2. Check Standards and Controls

```{r}
sc_controls_plot(nh3_run)
```
NOTE: The two sharp spikes in BLNK were due to the sample cup going empty, not analysis errors. I rerun the plot below with those two points removed.

```{r}
nh3_controls_plot <- 
nh3_run$controls %>% mutate(Concentration = ifelse(SampleID == "BLNK" & Concentration > 0.25, NA, Concentration)) %>% 
  ggplot(aes(RunTime, Concentration, color = SampleID)) +
  geom_point() +
  geom_line() +
  geom_smooth(method = "lm", se = F) +
  ggtitle(label = paste(nh3_run$method, "- Controls", nh3_run$run_date)) + 
  xlab("Runtime") +
  ylab("Concentration") +
  #theme(legend.position = "top") +
  geom_hline(yintercept = unique(nh3_run$controls$Nominal), linetype = 2)
ggplotly(nh3_controls_plot) %>% 
  layout(legend = list(orientation = "h", y = 1.05))
```



```{r}
nh3_midcheck_pctrec <- 
  nh3_run$controls %>% 
  filter(SampleID == "CCV5") %>% 
  ggplot(aes(RunTime, Recovery_PRD)) +
  geom_point() + 
  geom_line() +
  geom_hline(yintercept = c(100, 90, 110), linetype = c(1,2,2), color = "green") +
  labs(title = paste(nh3_run$method, "- Midrange Check % Recovery ", nh3_run$run_date))
ggplotly(nh3_midcheck_pctrec)

```



The controls were not great. I'll come back to this.

3. Duplicates analysis

  a. Absolute difference over time
  
```{r}
sc_absdups_plot(nh3_run)
```


All three together for comparison

```{r, fig.height=10, fig.width=10}
sc_qc_multiplot_fun(nh3_run)
```

### Data quality and corrections assessment

Assessment:

Mid-range check standard and blanks were outside of control parameters (+/- 10% recovery) starting at RunTime = 14:08:39. For the duration of the run, the controls remained mostly consistent at ~0.10 mg/L less than their nominal values. The two sharp spikes in BLNK were due to the blank sample cup going empty, not because of analysis errors. There was a conspicuous spike in both CCV5 and BLNK around 14:24 to 14:26. Dups varied by less than 0.1 mg/L with the exception of SampleID: 1627_Lac_2017-11-17_IN.

Corrections and reruns:

* Due to the spike in CCV5 and BLNK: rerun all samples between RunTime 14:17 and 14:33.
* Due to the spike in duplicate analysis at 14:44: rerun just sample 1627_Lac_2017-11-17_IN. CCV5 before and after this sample was consistent, which to me indicates that this was an isolated incident, perhaps some contamination in the cuvette. To assess this, flag the other samples in this set (between RunTime 14:41 and 14:48) for followup. 
* For all other samples after RunTime 14:08:39, apply a correction equivalent to the average absolute difference from nominal values for CCV5 (excluding the small spike at RunTime = 15:18:52) and BLNK (excluding errors from empty samples). 

Calculate correction factor
```{r}
corr_fact <- 
nh3_run$controls %>% 
  mutate(qc_diff = Nominal - Concentration) %>% 
  filter(SampleID != "CCB",
         RunTime >= ymd_hms("2018-08-21 14:08:39"),
         qc_diff > 0) %>% # filters out spikes in CCV5 and BLNK 
  summarise(correction = mean(qc_diff)) %>% 
  .$correction
corr_fact
```

Flag samples and apply correction factor
```{r}
nh3_results_final <- 
nh3_run$result %>% 
  mutate(qc_flag = ifelse(RunTime >= ymd_hms("2018-08-21 14:17:00") & RunTime <= ymd_hms("2018-08-21 14:33:00") |
                            grepl("1627_Lac_2017-11-17", SampleID), 
                          "rerun", NA)) %>% 
  mutate(qc_flag = ifelse(RunTime >= ymd_hms("2018-08-21 14:41:00") & RunTime <= ymd_hms("2018-08-21 14:48:00") & is.na(qc_flag), 
                          "correction_factor; followup", qc_flag)) %>% 
  mutate(qc_flag = ifelse(RunTime >= ymd_hms("2018-08-21 14:08:39") & is.na(qc_flag), "correction_factor", qc_flag),
         concentration_corrected = ifelse(grepl("correction_factor", qc_flag), Concentration + corr_fact, NA),
         concentration_final = ifelse(is.na(concentration_corrected) & qc_flag != "rerun" | is.na(qc_flag), Concentration, concentration_corrected))
nh3_results_final
```

Add metadata and export to DB

```{r}
nh3_results_final %>% 
  separate(SampleID, into = c("ws_info_id", "reservoir", "datetime", "site", "depth_nominal"), sep = "_", remove = FALSE) %>% 
  mutate(method = "WNHA",
         analyte = "nh3",
         date_col = date(datetime)) %>% 
  write.xlsx2(file = file.path("produced_data", "nh3_results_final_20180821.xlsx"), showNA = FALSE)
  
```

## SMPL - Phosphate

### Import and Process excel workbook

```{r}
po4_run <- smartchem_rawprocessor(filename = filenames_raw[2])
po4_run
```

### Quality Control Checks

1. Calibration curve

```{r}
sc_cal_curve_plot(po4_run)
```

2. Check Standards and Controls

```{r}
sc_controls_plot(po4_run)
```


```{r}
po4_midcheck_pctrec <- 
  po4_run$controls %>% 
  filter(SampleID == "CCV1") %>% 
  ggplot(aes(RunTime, Recovery_PRD)) +
  geom_point() + 
  geom_line() +
  geom_hline(yintercept = c(100, 90, 110), linetype = c(1,2,2), color = "green") +
  labs(title = paste(po4_run$method, "- Midrange Check % Recovery ", po4_run$run_date))
ggplotly(po4_midcheck_pctrec)

```


3. Duplicates analysis

  a. Absolute difference over time
  
```{r}
sc_absdups_plot(po4_run)
```


All three together for comparison

```{r, fig.height=10, fig.width=10}
sc_qc_multiplot_fun(po4_run)
```

### Data quality and corrections assessment

Overall, controls were good for this run. Dup for SampleID = 1623_Lac_2017-10-24_...IN (position = 36) was abnormally high and outside of the calibration range. The non-dup sample was on the upper end of the cal curve at ~0.09 mg/L. I'll flag this one for follow-up as a potential rerun if that is deemed necessary.

Flag one sample, add metadata, and export for db
```{r}
po4_run$result %>% 
  mutate(qc_flag = ifelse(Position == 36, "follow-up: high variation in duplicate", NA)) %>% 
  separate(SampleID, into = c("ws_info_id", "reservoir", "datetime", "site", "depth_nominal"), sep = "_", remove = FALSE) %>% 
  mutate(method = po4_run$method,
         analyte = "po4",
         date_col = date(datetime),
         filename = "po4_results_final_20180821.xlsx") %>% 
  write.xlsx2(file = file.path("produced_data", "po4_results_final_20180821.xlsx"), showNA = FALSE)

```




## BNO2 - Nitrite

### Import and Process excel workbook

```{r}
no2_run <- smartchem_rawprocessor(filename = filenames_raw[3])
no2_run
```

### Quality Control Checks

1. Calibration curve

```{r}
sc_cal_curve_plot(no2_run)
```

2. Check Standards and Controls

```{r}
sc_controls_plot(no2_run)
```


```{r}
no2_midcheck_pctrec <- 
  no2_run$controls %>% 
  filter(SampleID == "CCN2") %>% 
  ggplot(aes(RunTime, Recovery_PRD)) +
  geom_point() + 
  geom_line() +
  geom_hline(yintercept = c(100, 90, 110), linetype = c(1,2,2), color = "green") +
  labs(title = paste(no2_run$method, "- Midrange Check % Recovery ", no2_run$run_date))
ggplotly(no2_midcheck_pctrec)

```


3. Duplicates analysis

  a. Absolute difference over time
  
```{r}
sc_absdups_plot(no2_run)
```


All three together for comparison

```{r, fig.height=10, fig.width=10}
sc_qc_multiplot_fun(no2_run)
```

### Data quality and corrections assessment

Overall, controls were good for this run. The one peak in the BLNK set was due to the sample cup running low. 

Add metadata, and export for db
```{r}
no2_run$result %>% 
  separate(SampleID, into = c("ws_info_id", "reservoir", "datetime", "site", "depth_nominal"), sep = "_", remove = FALSE) %>% 
  mutate(method = no2_run$method,
         analyte = "no2",
         date_col = date(datetime),
         filename = "no2_results_final_20180822.xlsx") %>% 
  write.xlsx2(file = file.path("produced_data", "no2_results_final_20180822.xlsx"), showNA = FALSE)

```




## BNO3 - Nitrate

### Import and Process excel workbook

```{r}
no3_run <- smartchem_rawprocessor(filename = filenames_raw[4])
no3_run
```

### Quality Control Checks

1. Calibration curve

```{r}
sc_cal_curve_plot(no3_run)
```

2. Check Standards and Controls

```{r}
sc_controls_plot(no3_run)
```


```{r}
no3_midcheck_pctrec <- 
  no3_run$controls %>% 
  filter(SampleID == "CCN3") %>% 
  ggplot(aes(RunTime, Recovery_PRD)) +
  geom_point() + 
  geom_line() +
  geom_hline(yintercept = c(100, 90, 110), linetype = c(1,2,2), color = "green") +
  labs(title = paste(no3_run$method, "- Midrange Check % Recovery ", no3_run$run_date))
ggplotly(no3_midcheck_pctrec)

```


3. Duplicates analysis

  a. Absolute difference over time
  
```{r}
sc_absdups_plot(no3_run)
```


All three together for comparison

```{r, fig.height=10, fig.width=10}
sc_qc_multiplot_fun(no3_run)
```

Nitrate module coil efficiency

```{r}
sc_no3coileffic_plot(no3_run)
```

Mean coil efficiency (blue dotted line) = `r mean(no3_run$controls$coil_effic, na.rm = T)`


### Data quality and corrections assessment

Overall, controls were good for this run. 

Add metadata, and export for db
```{r}
no3_run$result %>% 
  separate(SampleID, into = c("ws_info_id", "reservoir", "datetime", "site", "depth_nominal"), sep = "_", remove = FALSE) %>% 
  mutate(method = no3_run$method,
         analyte = "no3",
         date_col = no3_run$run_date,
         filename = "no3_results_final_20180823.xlsx") %>% 
  write.xlsx2(file = file.path("produced_data", "no3_results_final_20180823.xlsx"), showNA = FALSE)

```


